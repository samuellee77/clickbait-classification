{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, EarlyStoppingCallback)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c754700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhsl023\u001b[0m (\u001b[33mhsl023-uc-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c103dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline  clickbait\n",
      "0  !Sdrawkcab: Missy Elliott, the Beatles and the...          0\n",
      "1  \"Apprentice\" contestant sues Trump for defamation          0\n",
      "2  \"Big morale boost\": George H.W. Bush tweets im...          0\n",
      "3  \"Bring it on\": Students sue Trump administrati...          0\n",
      "4  \"God made me bulletproof,\" oft-shot rapper Yun...          0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa1a41",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Text cleaning\n",
    "- Lowercase\n",
    "- Remove URLs, punctuation, stopwords if necessary\n",
    "- Optionally apply lemmatization/stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab79085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sdrawkcab missy elliott the beatles and the jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apprentice contestant sues trump for defamation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big morale boost george hw bush tweets image w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bring it on students sue trump administration ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>god made me bulletproof oftshot rapper yung ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53024</th>\n",
       "      <td>flip or flop  to end in  following hosts split</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53025</th>\n",
       "      <td>groundhog day  broadway musical giving away t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53026</th>\n",
       "      <td>scientific racism  is on the rise on the righ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53027</th>\n",
       "      <td>the walking dead  star to play the punisher i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53028</th>\n",
       "      <td>bn bank bailout in ireland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53029 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0      sdrawkcab missy elliott the beatles and the jo...          0\n",
       "1        apprentice contestant sues trump for defamation          0\n",
       "2      big morale boost george hw bush tweets image w...          0\n",
       "3      bring it on students sue trump administration ...          0\n",
       "4      god made me bulletproof oftshot rapper yung ma...          0\n",
       "...                                                  ...        ...\n",
       "53024     flip or flop  to end in  following hosts split          0\n",
       "53025   groundhog day  broadway musical giving away t...          0\n",
       "53026   scientific racism  is on the rise on the righ...          0\n",
       "53027   the walking dead  star to play the punisher i...          0\n",
       "53028                         bn bank bailout in ireland          0\n",
       "\n",
       "[53029 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    return text\n",
    "df['headline'] = df['headline'].apply(clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74fd0f",
   "metadata": {},
   "source": [
    "We will preprocess the dataset using natural language processing techniques. The major preprocessing steps include text cleaning and tokenization, where we would like to convert text to lowercase and remove punctuation, special characters, and digits. However, since special characters such as exclamation marks and question marks are usually used in clickbait titles, we should take these into consideration as they can be impactful indicators. In addition, in the tokenization step, we will split headlines into tokens using the same tokenizer as the target language model, such as BERTTokenizer. Then, the data will be split into train, test, and validation sets. The exact percentage of each set will be determined later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec640a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['headline'], df['clickbait'], test_size=0.2, random_state=42, stratify=df['clickbait']\n",
    ")\n",
    "train_df = pd.DataFrame({'headline': X_train, 'clickbait': y_train})\n",
    "test_df = pd.DataFrame({'headline': X_test, 'clickbait': y_test})\n",
    "small_train = train_df[:1000]\n",
    "small_eval = test_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9739b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae78d4747e147ba9db9367b0a09dbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabf31afb85a4fd88694910b13f32049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds = Dataset.from_pandas(small_train[[\"headline\",\"clickbait\"]]).map(tokenize, batched=True)\n",
    "test_ds  = Dataset.from_pandas(small_eval[[\"headline\",\"clickbait\"]]).map(tokenize, batched=True)\n",
    "train_ds = train_ds.rename_column(\"clickbait\", \"labels\")\n",
    "test_ds  = test_ds.rename_column(\"clickbait\", \"labels\")\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c66af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0:\"not_clickbait\", 1:\"clickbait\"},\n",
    "    label2id={\"not_clickbait\":0, \"clickbait\":1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # returns precision, recall, f1 for each label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"  # use \"macro\" if multiclass\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6cb437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={                 # <= put your hyperparameters here\n",
    "        \"run_name\": \"test-run\",\n",
    "        \"max_length\": 128,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_train\": 16,\n",
    "        \"batch_eval\": 32,\n",
    "        \"num_epochs\": 3,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"seed\": 42\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4373b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,       \n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=cfg[\"run_name\"],\n",
    "    learning_rate=cfg[\"learning_rate\"],\n",
    "    per_device_train_batch_size=cfg[\"batch_train\"],\n",
    "    per_device_eval_batch_size=cfg[\"batch_eval\"],\n",
    "    num_train_epochs=cfg[\"num_epochs\"],\n",
    "    weight_decay=cfg[\"weight_decay\"],\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d647af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5db1e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.870529</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0.838120</td>\n",
       "      <td>0.853723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.890669</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.892655</td>\n",
       "      <td>0.825065</td>\n",
       "      <td>0.857531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.899250</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.890141</td>\n",
       "      <td>0.825065</td>\n",
       "      <td>0.856369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.0016301620391902154, metrics={'train_runtime': 105.6081, 'train_samples_per_second': 28.407, 'train_steps_per_second': 1.79, 'total_flos': 99350548992000.0, 'train_loss': 0.0016301620391902154, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7782bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cse151/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41008061170578003, 'eval_accuracy': 0.897, 'eval_runtime': 8.2833, 'eval_samples_per_second': 120.726, 'eval_steps_per_second': 15.091, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse151",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
